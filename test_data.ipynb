{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data-csvs/test_images_labeled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_dir=filepath, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            base_dir (string): path of base directory.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(os.path.join(filepath,csv_file))\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.base_dir, self.data_frame.iloc[idx, 0])\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        label = self.data_frame.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Map class names to numerical labels\n",
    "        self.class_to_label = {\n",
    "            'dew': 0,\n",
    "            'fogsmog': 1,\n",
    "            'frost': 2,\n",
    "            'glaze': 3,\n",
    "            'hail': 4,\n",
    "            'lightning': 5,\n",
    "            'rain': 6,\n",
    "            'rainbow': 7,\n",
    "            'rime': 8,\n",
    "            'sandstorm': 9,\n",
    "            'snow': 10\n",
    "        }\n",
    "        label = self.class_to_label[label]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(train_csv, val_csv, test_csv, base_dir=filepath, augment=False, balance_classes=False):\n",
    "    # Apply any transformations here\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # augment training data\n",
    "    if augment:\n",
    "        csv_output = os.path.join(filepath, 'data-csvs/augmented_dataset.csv')\n",
    "        augment_data(train_csv, csv_output, output_folder='augmented_dataset')\n",
    "        train_csv = csv_output\n",
    "\n",
    "    # balance the classes\n",
    "    if balance_classes:\n",
    "        csv_output = os.path.join(filepath, 'data-csvs/balanced_augmented_dataset.csv')\n",
    "        balance_dataset(train_csv, csv_output, output_folder='balanced_augmented_dataset')\n",
    "        train_csv = csv_output\n",
    "\n",
    "    train_dataset = CustomDataset(csv_file=train_csv, base_dir=base_dir, transform=transform)\n",
    "    val_dataset = CustomDataset(csv_file=val_csv, base_dir=base_dir, transform=transform)\n",
    "    test_dataset = CustomDataset(csv_file=test_csv, base_dir=base_dir, transform=transform)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def setup_dataloaders(train_csv=filepath+'/data-csvs/train_images_labeled.csv', val_csv=filepath+'/data-csvs/valid_images_labeled.csv', test_csv=filepath+'/data-csvs/test_images_labeled.csv', batch_size=32, augment=False, balance_classes=False):\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(train_csv, val_csv, test_csv, augment=augment, balance_classes=balance_classes)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "def test_model(model, test_dl, nameOfModel):\n",
    "    \"\"\"\n",
    "    Function to run the model on the test data, calculate evaluation metrics,\n",
    "    and visualize performance.\n",
    "\n",
    "    Parameters:\n",
    "        model: PyTorch model to be tested.\n",
    "        test_dl: DataLoader containing test data.\n",
    "        pathtocsv: Path to the CSV file containing test data labels.\n",
    "        nameOfModel: Name of the model to be used in file name for test results.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store predictions and ground truth labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate over batches in the test DataLoader\n",
    "    for inputs, labels in test_dl:\n",
    "        # Forward pass to obtain predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        # Convert outputs to probabilities\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        # Get predicted labels (class with highest probability)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        # Append predictions and labels to lists\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Write evaluation metrics to file\n",
    "    with open(nameOfModel+'_evaluation.txt', 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"Precision: {precision}\\n\")\n",
    "        f.write(f\"Recall: {recall}\\n\")\n",
    "        f.write(f\"F1 Score: {f1}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Print message indicating completion\n",
    "    print(\"Evaluation completed. Evaluation metrics and plots saved to files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPI540",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
